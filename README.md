# üìò COMPENDIO: CLUSTERING EN PYTHON

---

## 1Ô∏è‚É£ ESTANDARIZACI√ìN

### ¬øCu√°ndo estandarizar?

| Situaci√≥n | ¬øEstandarizar? |
|-----------|---------------|
| Variables con diferentes unidades (edad, salario, altura) | ‚úÖ S√ç |
| Variables con misma unidad (temp1, temp2, temp3) | ‚ùå NO |
| Escalas muy diferentes (1-10 vs 1000-10000) | ‚úÖ S√ç |
| Datos binarios con Jaccard/Simple Matching | ‚ùå NO |
| Distancia de Mahalanobis | ‚ùå NO necesario |

### C√≥digo:

```python
from sklearn.preprocessing import StandardScaler

# Revisar escalas
print(df.describe())

# Estandarizar (media=0, std=1)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Usar datos estandarizados
Z = linkage(X_scaled, method='ward')
```

---

## 2Ô∏è‚É£ DECIDIR N√öMERO DE CLUSTERS (k)

### M√©todos disponibles:

### **A. Inspecci√≥n visual del dendrograma**
- Busca **saltos grandes** en altura
- Corta donde hay mayor diferencia vertical

### **B. M√©todo del Codo (Elbow Method)**

```python
from scipy.cluster.hierarchy import linkage, fcluster
import matplotlib.pyplot as plt

Z = linkage(X, method='ward')
varianzas = []

for k in range(1, 8):
    clusters = fcluster(Z, t=k, criterion='maxclust')
    varianza_total = 0
    for cluster_id in range(1, k+1):
        puntos = X[clusters == cluster_id]
        if len(puntos) > 0:
            centroide = puntos.mean(axis=0)
            varianza_total += ((puntos - centroide)**2).sum()
    varianzas.append(varianza_total)

plt.plot(range(1, 8), varianzas, 'bo-')
plt.xlabel('k')
plt.ylabel('Varianza intra-cluster')
plt.title('M√©todo del Codo')
plt.show()
```

**Interpretaci√≥n:** Elige k donde la curva hace "codo" (cambio de pendiente).

---

### **C. Coeficiente de Silueta** ‚≠ê (m√°s usado)

```python
from sklearn.metrics import silhouette_score
from scipy.cluster.hierarchy import fcluster

Z = linkage(X, method='ward')

for k in range(2, 6):
    clusters = fcluster(Z, t=k, criterion='maxclust')
    score = silhouette_score(X, clusters)
    print(f'k={k}: Silueta = {score:.3f}')
```

**Interpretaci√≥n:**
- **1.0**: clusters perfectos
- **0.5-0.7**: buena separaci√≥n
- **< 0.3**: clusters d√©biles
- **Negativo**: mala asignaci√≥n

**Elige el k con mayor silueta.**

---

### **D. Otros √≠ndices**

```python
from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score

for k in range(2, 6):
    clusters = fcluster(Z, t=k, criterion='maxclust')
    ch = calinski_harabasz_score(X, clusters)
    db = davies_bouldin_score(X, clusters)
    print(f'k={k}: Calinski-Harabasz={ch:.2f}, Davies-Bouldin={db:.3f}')
```

- **Calinski-Harabasz**: M√°s alto = mejor
- **Davies-Bouldin**: M√°s bajo = mejor

---

## 3Ô∏è‚É£ CORTAR DENDROGRAMA

### **M√©todo 1: Por distancia**

```python
from scipy.cluster.hierarchy import fcluster

# Cortar en una distancia espec√≠fica
clusters = fcluster(Z, t=7, criterion='distance')

# Visualizar corte
plt.axhline(y=7, color='red', linestyle='--', label='Corte')
```

**Cu√°ndo usar:** Conoces la distancia de corte deseada.

---

### **M√©todo 2: Por n√∫mero de clusters** ‚≠ê (m√°s com√∫n)

```python
# Especificar cu√°ntos clusters quieres
k = 3
clusters = fcluster(Z, t=k, criterion='maxclust')

# Calcular distancia de corte autom√°ticamente
if k > 1:
    distancia_corte = (Z[-(k-1), 2] + Z[-k, 2]) / 2
else:
    distancia_corte = Z[-1, 2]

# Visualizar
plt.axhline(y=distancia_corte, color='red', linestyle='--')
```

**Cu√°ndo usar:** Sabes cu√°ntos clusters necesitas (m√°s intuitivo).

---

### **M√©todo 3: Colorear autom√°ticamente**

```python
# Dendrograma con colores por cluster
dendrogram(Z, 
          color_threshold=distancia_corte,  # Colorea autom√°ticamente
          above_threshold_color='gray')
```

**Resultado:** Cada cluster tiene un color diferente en el dendrograma.

---

## 4Ô∏è‚É£ CAMBIAR N√öMERO DE CLUSTERS Y VER DENDROGRAMA

```python
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
import matplotlib.pyplot as plt

# 1. Hacer clustering UNA VEZ
Z = linkage(X, method='ward')

# 2. CAMBIAR ESTE N√öMERO
k = 3  # ‚Üê N√∫mero de clusters deseado

# 3. Calcular distancia de corte
if k > 1:
    distancia_corte = (Z[-(k-1), 2] + Z[-k, 2]) / 2
else:
    distancia_corte = Z[-1, 2]

# 4. Dibujar dendrograma coloreado
plt.figure(figsize=(10, 6))
dendrogram(Z, 
          labels=labels,
          color_threshold=distancia_corte,
          above_threshold_color='gray')
plt.axhline(y=distancia_corte, color='red', linestyle='--', label=f'k={k}')
plt.title(f'Dendrograma con {k} Clusters')
plt.ylabel('Distancia')
plt.legend()
plt.show()

# 5. Asignar clusters
clusters = fcluster(Z, t=k, criterion='maxclust')
```

---

## 5Ô∏è‚É£ RESUMEN DE FUNCIONES CLAVE

### **CLUSTERING JER√ÅRQUICO**

```python
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
from scipy.spatial.distance import pdist

# ---------- DATOS NUM√âRICOS ----------
# Calcular distancias
distancias = pdist(X, metric='euclidean')  # 'manhattan', 'minkowski', etc.

# O directamente:
Z = linkage(X, method='ward', metric='euclidean')
# M√©todos: 'single', 'complete', 'average', 'ward', 'centroid'

# ---------- DATOS BINARIOS ----------
# Calcular similitud Jaccard y convertir a distancia
distancias = pdist(X_binario, metric='jaccard')
Z = linkage(distancias, method='average')

# ---------- DENDROGRAMA ----------
dendrogram(Z, labels=['A', 'B', 'C'])
plt.show()

# ---------- ASIGNAR CLUSTERS ----------
# Por distancia
clusters = fcluster(Z, t=7, criterion='distance')

# Por n√∫mero de clusters
clusters = fcluster(Z, t=3, criterion='maxclust')
```

---

### **K-MEANS**

```python
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# ---------- ESTANDARIZAR (recomendado) ----------
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ---------- CLUSTERING ----------
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
clusters = kmeans.fit_predict(X_scaled)

# ---------- RESULTADOS ----------
centroides = kmeans.cluster_centers_  # Centroides
inercia = kmeans.inertia_  # Suma de distancias al cuadrado

# ---------- M√âTODO DEL CODO ----------
inertias = []
for k in range(1, 8):
    km = KMeans(n_clusters=k, random_state=42, n_init=10)
    km.fit(X_scaled)
    inertias.append(km.inertia_)

plt.plot(range(1, 8), inertias, 'bo-')
plt.xlabel('k')
plt.ylabel('Inercia')
plt.title('M√©todo del Codo - K-means')
plt.show()
```

---

### **M√âTRICAS DE DISTANCIA DISPONIBLES**

```python
from scipy.spatial.distance import pdist

# ---------- DATOS NUM√âRICOS ----------
pdist(X, metric='euclidean')    # Euclidiana (m√°s com√∫n)
pdist(X, metric='manhattan')    # Manhattan / City Block
pdist(X, metric='minkowski', p=3)  # Minkowski (generalizaci√≥n)
pdist(X, metric='chebyshev')    # Chebyshev
pdist(X, metric='cosine')       # Coseno
pdist(X, metric='correlation')  # Correlaci√≥n

# ---------- DATOS BINARIOS ----------
pdist(X, metric='jaccard')      # Jaccard (ignora 0-0)
pdist(X, metric='dice')         # Dice
pdist(X, metric='hamming')      # Hamming
pdist(X, metric='matching')     # Simple Matching (considera 0-0)
```

---

## 6Ô∏è‚É£ TIPS IMPORTANTES

### ‚úÖ **Clustering Jer√°rquico**

1. **Estandariza si las variables tienen diferentes escalas**
2. **Ward es el m√©todo de enlace m√°s usado** (minimiza varianza)
3. **Single linkage** tiende a crear "cadenas" (clusters alargados)
4. **Complete linkage** tiende a crear clusters compactos
5. **Para datos binarios:** usa Jaccard (ignora 0-0) o Simple Matching (considera 0-0)

### ‚úÖ **K-means**

1. **SIEMPRE estandariza** antes de K-means
2. **Usa `random_state`** para reproducibilidad
3. **Usa `n_init=10`** (ejecuta 10 veces con diferentes inicializaciones)
4. K-means es **sensible a outliers**
5. Funciona mejor con **clusters esf√©ricos**

### ‚úÖ **General**

1. **No existe "el k correcto"** ‚Üí prueba varios y eval√∫a
2. **Silueta > 0.5** es buena se√±al
3. **Interpreta los resultados** ‚Üí ¬øtienen sentido pr√°ctico?
4. **Visualiza siempre** tus datos antes de clustering
5. **Compara m√©todos** (jer√°rquico vs k-means) para validar

---

## 7Ô∏è‚É£ TEMPLATE M√çNIMO PARA TAREAS

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
from scipy.spatial.distance import pdist
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

# ========== 1. CARGAR DATOS ==========
df = pd.read_csv('datos.csv')
X = df[['Var1', 'Var2']].values
labels = df['ID'].values

# ========== 2. ESTANDARIZAR (si es necesario) ==========
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ========== 3. CLUSTERING JER√ÅRQUICO ==========
Z = linkage(X_scaled, method='ward')

# Encontrar mejor k con silueta
for k in range(2, 6):
    clusters = fcluster(Z, t=k, criterion='maxclust')
    score = silhouette_score(X_scaled, clusters)
    print(f'k={k}: Silueta={score:.3f}')

# Elegir k y visualizar
k = 3
distancia_corte = (Z[-(k-1), 2] + Z[-k, 2]) / 2

plt.figure(figsize=(10, 6))
dendrogram(Z, labels=labels, color_threshold=distancia_corte)
plt.axhline(y=distancia_corte, color='red', linestyle='--')
plt.title(f'Dendrograma - {k} Clusters')
plt.show()

# ========== 4. K-MEANS ==========
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
clusters_km = kmeans.fit_predict(X_scaled)

# Visualizar
plt.scatter(X[:, 0], X[:, 1], c=clusters_km)
plt.scatter(kmeans.cluster_centers_[:, 0], 
           kmeans.cluster_centers_[:, 1], 
           c='red', marker='X', s=200)
plt.title('K-Means')
plt.show()
```

---

## 8Ô∏è‚É£ DECISIONES CLAVE - CHEATSHEET

| Pregunta | Respuesta |
|----------|-----------|
| **¬øEstandarizar?** | S√ç si escalas diferentes, NO si misma escala |
| **¬øQu√© m√©todo de enlace?** | Ward (m√°s usado), o Complete para clusters compactos |
| **¬øJaccard o Simple Matching?** | Jaccard si 0-0 no importan, Simple Matching si s√≠ |
| **¬øC√≥mo elegir k?** | Silueta + inspecci√≥n visual + interpretabilidad |
| **¬øJer√°rquico o K-means?** | Jer√°rquico para explorar, K-means si sabes k |
| **¬øQu√© distancia usar?** | Euclidiana (m√°s com√∫n), Jaccard para binarios |

---

## 9Ô∏è‚É£ FLUJO COMPLETO DEL PROCESO

```
1. DATOS ORIGINALES
        ‚Üì
2. ¬øNum√©ricas o binarias?
        ‚Üì
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚Üì                   ‚Üì
Num√©ricas          Binarias
   ‚Üì                   ‚Üì
¬øEscalas           Similitud
diferentes?        (Jaccard/Simple Matching)
   ‚Üì                   ‚Üì
S√ç ‚Üí Estandarizar  Convertir a distancia
   ‚Üì                   ‚Üì
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚Üì
3. CAPACIDAD DE CALCULAR DISTANCIAS
        ‚Üì
4. ELIGE M√âTODO DE CLUSTERING
        ‚Üì
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚Üì                       ‚Üì
Jer√°rquico           No jer√°rquico
   ‚Üì                       ‚Üì
‚îå‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îê               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚Üì     ‚Üì               ‚Üì         ‚Üì
Aglom Divis       K-means    DBSCAN
   ‚Üì                   ‚Üì
M√©todo de enlace   Elegir k
(single, complete, M√©todo del codo
average, ward)     o Silueta
   ‚Üì                   ‚Üì
Dendrograma        Visualizar
   ‚Üì                   ‚Üì
Elegir k           Evaluar
   ‚Üì                   ‚Üì
Asignar clusters   Resultados
```

---

## üîü MEDIDAS DE SIMILITUD PARA DATOS BINARIOS

### **Coeficiente de Jaccard** (el m√°s usado)

**F√≥rmula:** `s = a / (a + b + c)`

- **a**: coincidencias 1-1
- **b**: discrepancias (0-1)
- **c**: discrepancias (1-0)
- **Ignora d** (coincidencias 0-0)

**Cu√°ndo usar:** Especies en sitios, productos comprados, palabras en documentos (cuando NO tener algo en com√∫n NO significa similitud)

```python
distancias = pdist(X_binario, metric='jaccard')
```

---

### **Simple Matching**

**F√≥rmula:** `s = (a + d) / p`

- **Considera d** (coincidencias 0-0)

**Cu√°ndo usar:** S√≠ntomas m√©dicos, caracter√≠sticas de casa, test binarios (cuando ausencias compartidas S√ç son informativas)

```python
def simple_matching_distance(u, v):
    coincidencias = np.sum(u == v)
    similitud = coincidencias / len(u)
    return 1 - similitud

distancias = pdist(X_binario, metric=simple_matching_distance)
```

---

### **Tabla de contingencia**

Cuando comparas dos objetos i y k:

```
           Objeto k
        1     0    Total
i   1   a     b    a+b
    0   c     d    c+d
Total  a+c   b+d    p
```

**Conversi√≥n de similitud a distancia:**

```python
d = sqrt(2 * (1 - s))
```

---

## 1Ô∏è‚É£1Ô∏è‚É£ EJEMPLO COMPLETO PASO A PASO

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

# ========== DATOS ==========
datos = {
    'ID': ['A', 'B', 'C', 'D', 'E'],
    'Edad': [25, 27, 45, 50, 52],
    'Salario': [30000, 32000, 60000, 65000, 70000]
}
df = pd.DataFrame(datos)
print(df.describe())  # Verificar escalas

# ========== PREPARAR ==========
X = df[['Edad', 'Salario']].values
labels = df['ID'].values

# Estandarizar (escalas muy diferentes)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ========== CLUSTERING ==========
Z = linkage(X_scaled, method='ward')

# ========== ELEGIR k CON SILUETA ==========
print("\nCoeficientes de Silueta:")
for k in range(2, 5):
    clusters = fcluster(Z, t=k, criterion='maxclust')
    score = silhouette_score(X_scaled, clusters)
    print(f'k={k}: {score:.3f}')

# ========== VISUALIZAR CON k=2 ==========
k = 2
distancia_corte = (Z[-(k-1), 2] + Z[-k, 2]) / 2

plt.figure(figsize=(10, 6))
dendrogram(Z, 
          labels=labels,
          color_threshold=distancia_corte,
          above_threshold_color='gray')
plt.axhline(y=distancia_corte, color='red', linestyle='--', linewidth=2)
plt.title(f'Dendrograma - {k} Clusters (Ward)')
plt.ylabel('Distancia')
plt.show()

# ========== ASIGNAR Y MOSTRAR ==========
clusters = fcluster(Z, t=k, criterion='maxclust')
df['Cluster'] = clusters
print("\nAsignaci√≥n de clusters:")
print(df)
```

---

## 1Ô∏è‚É£2Ô∏è‚É£ VALIDACI√ìN DE CLUSTERS ‚≠ê

### **¬øPor qu√© validar?**

La validaci√≥n asegura que tus clusters sean:
- ‚úÖ **Representativos**: reflejan estructura real en los datos
- ‚úÖ **Generalizables**: no son resultado de ruido o azar
- ‚úÖ **Estables**: se mantienen con peque√±os cambios en datos/par√°metros
- ‚úÖ **Robustos**: no dependen excesivamente de una variable espec√≠fica

---

### **M√©todo 1: Diferentes Medidas de Distancia**

Prueba m√∫ltiples m√©tricas y compara resultados.

```python
from scipy.cluster.hierarchy import linkage, fcluster
from sklearn.metrics import adjusted_rand_score
from scipy.spatial.distance import pdist

# Clustering con diferentes distancias
metricas = ['euclidean', 'manhattan', 'cosine']
resultados = {}

for metrica in metricas:
    distancias = pdist(X_scaled, metric=metrica)
    Z = linkage(distancias, method='ward')
    clusters = fcluster(Z, t=3, criterion='maxclust')
    resultados[metrica] = clusters

# Comparar con Adjusted Rand Index (ARI)
print("Comparaci√≥n de m√©tricas de distancia:")
for i, metrica1 in enumerate(metricas):
    for metrica2 in metricas[i+1:]:
        ari = adjusted_rand_score(resultados[metrica1], resultados[metrica2])
        print(f'{metrica1} vs {metrica2}: ARI = {ari:.3f}')
```

**Interpretaci√≥n:**
- **ARI > 0.7**: clusters estables entre m√©tricas ‚úÖ
- **ARI < 0.5**: resultados inconsistentes, revisar datos ‚ö†Ô∏è

---

### **M√©todo 2: Diferentes M√©todos de Clustering**

Compara jer√°rquico (varios enlaces) vs K-means.

```python
from sklearn.cluster import KMeans
from sklearn.metrics import adjusted_rand_score, silhouette_score

# 1. Clustering jer√°rquico con diferentes m√©todos
metodos = ['ward', 'complete', 'average', 'single']
resultados_jerarquico = {}

for metodo in metodos:
    Z = linkage(X_scaled, method=metodo)
    clusters = fcluster(Z, t=3, criterion='maxclust')
    resultados_jerarquico[metodo] = clusters
    sil = silhouette_score(X_scaled, clusters)
    print(f'{metodo}: Silueta = {sil:.3f}')

# 2. K-means
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
clusters_kmeans = kmeans.fit_predict(X_scaled)
sil_kmeans = silhouette_score(X_scaled, clusters_kmeans)
print(f'K-means: Silueta = {sil_kmeans:.3f}')

# 3. Comparar todos vs todos
print("\nComparaci√≥n entre m√©todos:")
for metodo in metodos:
    ari = adjusted_rand_score(resultados_jerarquico[metodo], clusters_kmeans)
    print(f'{metodo} vs K-means: ARI = {ari:.3f}')
```

**Interpretaci√≥n:**
- Si todos los m√©todos dan **resultados similares** ‚Üí alta confianza ‚úÖ
- Si resultados **muy diferentes** ‚Üí estructura d√©bil o datos complejos ‚ö†Ô∏è

---

### **M√©todo 3: Split-Half (Dividir Datos)**

Divide datos aleatoriamente y verifica consistencia.

```python
from sklearn.model_selection import train_test_split
from sklearn.metrics import adjusted_rand_score, silhouette_score

# Dividir datos en dos mitades aleatorias
X_1, X_2 = train_test_split(X_scaled, test_size=0.5, random_state=42)

# Clustering en cada mitad
Z_1 = linkage(X_1, method='ward')
Z_2 = linkage(X_2, method='ward')

clusters_1 = fcluster(Z_1, t=3, criterion='maxclust')
clusters_2 = fcluster(Z_2, t=3, criterion='maxclust')

# Comparar siluetas
sil_1 = silhouette_score(X_1, clusters_1)
sil_2 = silhouette_score(X_2, clusters_2)

print(f'Mitad 1: Silueta = {sil_1:.3f}')
print(f'Mitad 2: Silueta = {sil_2:.3f}')
print(f'Diferencia: {abs(sil_1 - sil_2):.3f}')
```

**Interpretaci√≥n:**
- **Diferencia < 0.2**: clusters estables ‚úÖ
- **Diferencia > 0.3**: clusters inestables, dependen de muestra espec√≠fica ‚ö†Ô∏è

---

### **M√©todo 4: Eliminar Variables Aleatoriamente**

Verifica que clusters no dependan de una sola variable.

```python
import numpy as np
from sklearn.metrics import adjusted_rand_score

# Clustering con TODAS las variables
Z_completo = linkage(X_scaled, method='ward')
clusters_completo = fcluster(Z_completo, t=3, criterion='maxclust')

# Clustering eliminando una variable aleatoria
n_vars = X_scaled.shape[1]
for i in range(n_vars):
    # Eliminar variable i
    X_sin_i = np.delete(X_scaled, i, axis=1)
    
    Z_parcial = linkage(X_sin_i, method='ward')
    clusters_parcial = fcluster(Z_parcial, t=3, criterion='maxclust')
    
    # Comparar
    ari = adjusted_rand_score(clusters_completo, clusters_parcial)
    print(f'Sin variable {i}: ARI = {ari:.3f}')
```

**Interpretaci√≥n:**
- **ARI > 0.7** al eliminar cualquier variable ‚Üí clusters robustos ‚úÖ
- **ARI < 0.5** al eliminar una variable ‚Üí clusters dependen de esa variable ‚ö†Ô∏è

---

### **M√©todo 5: M√∫ltiples Ejecuciones (K-means)**

K-means puede dar resultados diferentes por inicializaci√≥n aleatoria.

```python
from sklearn.cluster import KMeans
from sklearn.metrics import adjusted_rand_score
import numpy as np

# Ejecutar K-means 10 veces con diferentes random_state
resultados = []
for i in range(10):
    kmeans = KMeans(n_clusters=3, random_state=i, n_init=10)
    clusters = kmeans.fit_predict(X_scaled)
    resultados.append(clusters)

# Comparar todas las ejecuciones
aris = []
for i in range(len(resultados)):
    for j in range(i+1, len(resultados)):
        ari = adjusted_rand_score(resultados[i], resultados[j])
        aris.append(ari)

ari_promedio = np.mean(aris)
ari_min = np.min(aris)

print(f'ARI promedio entre ejecuciones: {ari_promedio:.3f}')
print(f'ARI m√≠nimo: {ari_min:.3f}')
```

**Interpretaci√≥n:**
- **ARI promedio > 0.9**: K-means muy estable ‚úÖ
- **ARI promedio < 0.7**: resultados inconsistentes, probar m√°s n_init ‚ö†Ô∏è

**Nota:** Por eso se recomienda `n_init=10` en K-means (ejecuta 10 veces autom√°ticamente).

---

### **üìã CHECKLIST DE VALIDACI√ìN**

Antes de reportar resultados, verifica:

- [ ] ‚úÖ **Similar con diferentes distancias** (ARI > 0.7)
- [ ] ‚úÖ **Similar con diferentes m√©todos** (jer√°rquico vs K-means)
- [ ] ‚úÖ **Similar al dividir datos** (diferencia silueta < 0.2)
- [ ] ‚úÖ **Robusto al eliminar variables** (ARI > 0.7)
- [ ] ‚úÖ **Estable en m√∫ltiples ejecuciones** (K-means ARI > 0.9)
- [ ] ‚úÖ **Silueta > 0.5** (buena separaci√≥n)
- [ ] ‚úÖ **Interpretable y con sentido pr√°ctico**

---

### **üéØ C√ìDIGO COMPLETO DE VALIDACI√ìN**

```python
import numpy as np
import pandas as pd
from scipy.cluster.hierarchy import linkage, fcluster
from scipy.spatial.distance import pdist
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, adjusted_rand_score
from sklearn.model_selection import train_test_split

# ========== PREPARAR DATOS ==========
df = pd.read_csv('datos.csv')
X = df[['Var1', 'Var2', 'Var3']].values
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

k = 3  # N√∫mero de clusters

# ========== 1. VALIDACI√ìN: DIFERENTES DISTANCIAS ==========
print("=== VALIDACI√ìN 1: Diferentes Distancias ===")
metricas = ['euclidean', 'manhattan', 'cosine']
resultados_dist = {}

for metrica in metricas:
    distancias = pdist(X_scaled, metric=metrica)
    Z = linkage(distancias, method='ward')
    clusters = fcluster(Z, t=k, criterion='maxclust')
    resultados_dist[metrica] = clusters
    sil = silhouette_score(X_scaled, clusters)
    print(f'{metrica}: Silueta = {sil:.3f}')

# Comparar m√©tricas
ari_dist = adjusted_rand_score(resultados_dist['euclidean'], 
                                 resultados_dist['manhattan'])
print(f'ARI (euclidean vs manhattan): {ari_dist:.3f}')

# ========== 2. VALIDACI√ìN: DIFERENTES M√âTODOS ==========
print("\n=== VALIDACI√ìN 2: Diferentes M√©todos ===")
Z_ward = linkage(X_scaled, method='ward')
clusters_ward = fcluster(Z_ward, t=k, criterion='maxclust')

kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
clusters_kmeans = kmeans.fit_predict(X_scaled)

ari_metodos = adjusted_rand_score(clusters_ward, clusters_kmeans)
print(f'ARI (Ward vs K-means): {ari_metodos:.3f}')

# ========== 3. VALIDACI√ìN: SPLIT-HALF ==========
print("\n=== VALIDACI√ìN 3: Split-Half ===")
X_1, X_2 = train_test_split(X_scaled, test_size=0.5, random_state=42)

Z_1 = linkage(X_1, method='ward')
clusters_1 = fcluster(Z_1, t=k, criterion='maxclust')
sil_1 = silhouette_score(X_1, clusters_1)

Z_2 = linkage(X_2, method='ward')
clusters_2 = fcluster(Z_2, t=k, criterion='maxclust')
sil_2 = silhouette_score(X_2, clusters_2)

print(f'Silueta Mitad 1: {sil_1:.3f}')
print(f'Silueta Mitad 2: {sil_2:.3f}')
print(f'Diferencia: {abs(sil_1 - sil_2):.3f}')

# ========== 4. VALIDACI√ìN: ELIMINAR VARIABLES ==========
print("\n=== VALIDACI√ìN 4: Robustez Variables ===")
Z_completo = linkage(X_scaled, method='ward')
clusters_completo = fcluster(Z_completo, t=k, criterion='maxclust')

for i in range(X_scaled.shape[1]):
    X_sin_i = np.delete(X_scaled, i, axis=1)
    Z_parcial = linkage(X_sin_i, method='ward')
    clusters_parcial = fcluster(Z_parcial, t=k, criterion='maxclust')
    ari = adjusted_rand_score(clusters_completo, clusters_parcial)
    print(f'Sin variable {i}: ARI = {ari:.3f}')

# ========== 5. VALIDACI√ìN: M√öLTIPLES EJECUCIONES K-MEANS ==========
print("\n=== VALIDACI√ìN 5: Estabilidad K-means ===")
resultados_km = []
for i in range(10):
    km = KMeans(n_clusters=k, random_state=i, n_init=10)
    clusters = km.fit_predict(X_scaled)
    resultados_km.append(clusters)

aris_km = []
for i in range(len(resultados_km)):
    for j in range(i+1, len(resultados_km)):
        ari = adjusted_rand_score(resultados_km[i], resultados_km[j])
        aris_km.append(ari)

print(f'ARI promedio: {np.mean(aris_km):.3f}')
print(f'ARI m√≠nimo: {np.min(aris_km):.3f}')

# ========== RESUMEN DE VALIDACI√ìN ==========
print("\n" + "="*50)
print("RESUMEN DE VALIDACI√ìN")
print("="*50)
print(f"‚úì Diferentes distancias: ARI = {ari_dist:.3f}")
print(f"‚úì Diferentes m√©todos: ARI = {ari_metodos:.3f}")
print(f"‚úì Split-half: Diferencia = {abs(sil_1 - sil_2):.3f}")
print(f"‚úì K-means estabilidad: ARI promedio = {np.mean(aris_km):.3f}")
print(f"‚úì Silueta final: {silhouette_score(X_scaled, clusters_ward):.3f}")
```

---

### **üîë CRITERIOS DE DECISI√ìN**

| M√©trica | Valor | Interpretaci√≥n |
|---------|-------|----------------|
| **ARI entre m√©todos** | > 0.7 | ‚úÖ Clusters estables |
|  | 0.5 - 0.7 | ‚ö†Ô∏è Estructura moderada |
|  | < 0.5 | ‚ùå Resultados inconsistentes |
| **Diferencia Silueta** | < 0.2 | ‚úÖ Clusters robustos |
| (Split-half) | 0.2 - 0.3 | ‚ö†Ô∏è Moderadamente estables |
|  | > 0.3 | ‚ùå Muy inestables |
| **Silueta absoluta** | > 0.7 | ‚úÖ Excelente separaci√≥n |
|  | 0.5 - 0.7 | ‚úÖ Buena separaci√≥n |
|  | 0.3 - 0.5 | ‚ö†Ô∏è Estructura d√©bil |
|  | < 0.3 | ‚ùå Clusters poco claros |

---

**FIN DEL COMPENDIO** üìò

---

**Autor:** Claude (Anthropic)  
**Fecha:** Noviembre 2025  
**Tema:** An√°lisis de Conglomerados en Python  
**Librer√≠as:** scipy, scikit-learn, pandas, numpy, matplotlib
