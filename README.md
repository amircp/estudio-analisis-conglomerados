# ğŸ“˜ COMPENDIO: CLUSTERING EN PYTHON

---

## 1ï¸âƒ£ ESTANDARIZACIÃ“N

### Â¿CuÃ¡ndo estandarizar?

| SituaciÃ³n | Â¿Estandarizar? |
|-----------|---------------|
| Variables con diferentes unidades (edad, salario, altura) | âœ… SÃ |
| Variables con misma unidad (temp1, temp2, temp3) | âŒ NO |
| Escalas muy diferentes (1-10 vs 1000-10000) | âœ… SÃ |
| Datos binarios con Jaccard/Simple Matching | âŒ NO |
| Distancia de Mahalanobis | âŒ NO necesario |

### CÃ³digo:

```python
from sklearn.preprocessing import StandardScaler

# Revisar escalas
print(df.describe())

# Estandarizar (media=0, std=1)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Usar datos estandarizados
Z = linkage(X_scaled, method='ward')
```

---

## 2ï¸âƒ£ DECIDIR NÃšMERO DE CLUSTERS (k)

### MÃ©todos disponibles:

### **A. InspecciÃ³n visual del dendrograma**
- Busca **saltos grandes** en altura
- Corta donde hay mayor diferencia vertical

### **B. MÃ©todo del Codo (Elbow Method)**

```python
from scipy.cluster.hierarchy import linkage, fcluster
import matplotlib.pyplot as plt

Z = linkage(X, method='ward')
varianzas = []

for k in range(1, 8):
    clusters = fcluster(Z, t=k, criterion='maxclust')
    varianza_total = 0
    for cluster_id in range(1, k+1):
        puntos = X[clusters == cluster_id]
        if len(puntos) > 0:
            centroide = puntos.mean(axis=0)
            varianza_total += ((puntos - centroide)**2).sum()
    varianzas.append(varianza_total)

plt.plot(range(1, 8), varianzas, 'bo-')
plt.xlabel('k')
plt.ylabel('Varianza intra-cluster')
plt.title('MÃ©todo del Codo')
plt.show()
```

**InterpretaciÃ³n:** Elige k donde la curva hace "codo" (cambio de pendiente).

---

### **C. Coeficiente de Silueta** â­ (mÃ¡s usado)

```python
from sklearn.metrics import silhouette_score
from scipy.cluster.hierarchy import fcluster

Z = linkage(X, method='ward')

for k in range(2, 6):
    clusters = fcluster(Z, t=k, criterion='maxclust')
    score = silhouette_score(X, clusters)
    print(f'k={k}: Silueta = {score:.3f}')
```

**InterpretaciÃ³n:**
- **1.0**: clusters perfectos
- **0.5-0.7**: buena separaciÃ³n
- **< 0.3**: clusters dÃ©biles
- **Negativo**: mala asignaciÃ³n

**Elige el k con mayor silueta.**

---

### **D. Otros Ã­ndices**

```python
from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score

for k in range(2, 6):
    clusters = fcluster(Z, t=k, criterion='maxclust')
    ch = calinski_harabasz_score(X, clusters)
    db = davies_bouldin_score(X, clusters)
    print(f'k={k}: Calinski-Harabasz={ch:.2f}, Davies-Bouldin={db:.3f}')
```

- **Calinski-Harabasz**: MÃ¡s alto = mejor
- **Davies-Bouldin**: MÃ¡s bajo = mejor

---

## 3ï¸âƒ£ CORTAR DENDROGRAMA

### **MÃ©todo 1: Por distancia**

```python
from scipy.cluster.hierarchy import fcluster

# Cortar en una distancia especÃ­fica
clusters = fcluster(Z, t=7, criterion='distance')

# Visualizar corte
plt.axhline(y=7, color='red', linestyle='--', label='Corte')
```

**CuÃ¡ndo usar:** Conoces la distancia de corte deseada.

---

### **MÃ©todo 2: Por nÃºmero de clusters** â­ (mÃ¡s comÃºn)

```python
# Especificar cuÃ¡ntos clusters quieres
k = 3
clusters = fcluster(Z, t=k, criterion='maxclust')

# Calcular distancia de corte automÃ¡ticamente
if k > 1:
    distancia_corte = (Z[-(k-1), 2] + Z[-k, 2]) / 2
else:
    distancia_corte = Z[-1, 2]

# Visualizar
plt.axhline(y=distancia_corte, color='red', linestyle='--')
```

**CuÃ¡ndo usar:** Sabes cuÃ¡ntos clusters necesitas (mÃ¡s intuitivo).

---

### **MÃ©todo 3: Colorear automÃ¡ticamente**

```python
# Dendrograma con colores por cluster
dendrogram(Z, 
          color_threshold=distancia_corte,  # Colorea automÃ¡ticamente
          above_threshold_color='gray')
```

**Resultado:** Cada cluster tiene un color diferente en el dendrograma.

---

## 4ï¸âƒ£ CAMBIAR NÃšMERO DE CLUSTERS Y VER DENDROGRAMA

```python
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
import matplotlib.pyplot as plt

# 1. Hacer clustering UNA VEZ
Z = linkage(X, method='ward')

# 2. CAMBIAR ESTE NÃšMERO
k = 3  # â† NÃºmero de clusters deseado

# 3. Calcular distancia de corte
if k > 1:
    distancia_corte = (Z[-(k-1), 2] + Z[-k, 2]) / 2
else:
    distancia_corte = Z[-1, 2]

# 4. Dibujar dendrograma coloreado
plt.figure(figsize=(10, 6))
dendrogram(Z, 
          labels=labels,
          color_threshold=distancia_corte,
          above_threshold_color='gray')
plt.axhline(y=distancia_corte, color='red', linestyle='--', label=f'k={k}')
plt.title(f'Dendrograma con {k} Clusters')
plt.ylabel('Distancia')
plt.legend()
plt.show()

# 5. Asignar clusters
clusters = fcluster(Z, t=k, criterion='maxclust')
```

---

## 5ï¸âƒ£ RESUMEN DE FUNCIONES CLAVE

### **CLUSTERING JERÃRQUICO**

```python
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
from scipy.spatial.distance import pdist

# ---------- DATOS NUMÃ‰RICOS ----------
# Calcular distancias
distancias = pdist(X, metric='euclidean')  # 'manhattan', 'minkowski', etc.

# O directamente:
Z = linkage(X, method='ward', metric='euclidean')
# MÃ©todos: 'single', 'complete', 'average', 'ward', 'centroid'

# ---------- DATOS BINARIOS ----------
# Calcular similitud Jaccard y convertir a distancia
distancias = pdist(X_binario, metric='jaccard')
Z = linkage(distancias, method='average')

# ---------- DENDROGRAMA ----------
dendrogram(Z, labels=['A', 'B', 'C'])
plt.show()

# ---------- ASIGNAR CLUSTERS ----------
# Por distancia
clusters = fcluster(Z, t=7, criterion='distance')

# Por nÃºmero de clusters
clusters = fcluster(Z, t=3, criterion='maxclust')
```

---

### **K-MEANS**

```python
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# ---------- ESTANDARIZAR (recomendado) ----------
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ---------- CLUSTERING ----------
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
clusters = kmeans.fit_predict(X_scaled)

# ---------- RESULTADOS ----------
centroides = kmeans.cluster_centers_  # Centroides
inercia = kmeans.inertia_  # Suma de distancias al cuadrado

# ---------- MÃ‰TODO DEL CODO ----------
inertias = []
for k in range(1, 8):
    km = KMeans(n_clusters=k, random_state=42, n_init=10)
    km.fit(X_scaled)
    inertias.append(km.inertia_)

plt.plot(range(1, 8), inertias, 'bo-')
plt.xlabel('k')
plt.ylabel('Inercia')
plt.title('MÃ©todo del Codo - K-means')
plt.show()
```

---

### **MÃ‰TRICAS DE DISTANCIA DISPONIBLES**

```python
from scipy.spatial.distance import pdist

# ---------- DATOS NUMÃ‰RICOS ----------
pdist(X, metric='euclidean')    # Euclidiana (mÃ¡s comÃºn)
pdist(X, metric='manhattan')    # Manhattan / City Block
pdist(X, metric='minkowski', p=3)  # Minkowski (generalizaciÃ³n)
pdist(X, metric='chebyshev')    # Chebyshev
pdist(X, metric='cosine')       # Coseno
pdist(X, metric='correlation')  # CorrelaciÃ³n

# ---------- DATOS BINARIOS ----------
pdist(X, metric='jaccard')      # Jaccard (ignora 0-0)
pdist(X, metric='dice')         # Dice
pdist(X, metric='hamming')      # Hamming
pdist(X, metric='matching')     # Simple Matching (considera 0-0)
```

---

## 6ï¸âƒ£ TIPS IMPORTANTES

### âœ… **Clustering JerÃ¡rquico**

1. **Estandariza si las variables tienen diferentes escalas**
2. **Ward es el mÃ©todo de enlace mÃ¡s usado** (minimiza varianza)
3. **Single linkage** tiende a crear "cadenas" (clusters alargados)
4. **Complete linkage** tiende a crear clusters compactos
5. **Para datos binarios:** usa Jaccard (ignora 0-0) o Simple Matching (considera 0-0)

### âœ… **K-means**

1. **SIEMPRE estandariza** antes de K-means
2. **Usa `random_state`** para reproducibilidad
3. **Usa `n_init=10`** (ejecuta 10 veces con diferentes inicializaciones)
4. K-means es **sensible a outliers**
5. Funciona mejor con **clusters esfÃ©ricos**

### âœ… **General**

1. **No existe "el k correcto"** â†’ prueba varios y evalÃºa
2. **Silueta > 0.5** es buena seÃ±al
3. **Interpreta los resultados** â†’ Â¿tienen sentido prÃ¡ctico?
4. **Visualiza siempre** tus datos antes de clustering
5. **Compara mÃ©todos** (jerÃ¡rquico vs k-means) para validar

---

## 7ï¸âƒ£ TEMPLATE MÃNIMO PARA TAREAS

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
from scipy.spatial.distance import pdist
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

# ========== 1. CARGAR DATOS ==========
df = pd.read_csv('datos.csv')
X = df[['Var1', 'Var2']].values
labels = df['ID'].values

# ========== 2. ESTANDARIZAR (si es necesario) ==========
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ========== 3. CLUSTERING JERÃRQUICO ==========
Z = linkage(X_scaled, method='ward')

# Encontrar mejor k con silueta
for k in range(2, 6):
    clusters = fcluster(Z, t=k, criterion='maxclust')
    score = silhouette_score(X_scaled, clusters)
    print(f'k={k}: Silueta={score:.3f}')

# Elegir k y visualizar
k = 3
distancia_corte = (Z[-(k-1), 2] + Z[-k, 2]) / 2

plt.figure(figsize=(10, 6))
dendrogram(Z, labels=labels, color_threshold=distancia_corte)
plt.axhline(y=distancia_corte, color='red', linestyle='--')
plt.title(f'Dendrograma - {k} Clusters')
plt.show()

# ========== 4. K-MEANS ==========
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
clusters_km = kmeans.fit_predict(X_scaled)

# Visualizar
plt.scatter(X[:, 0], X[:, 1], c=clusters_km)
plt.scatter(kmeans.cluster_centers_[:, 0], 
           kmeans.cluster_centers_[:, 1], 
           c='red', marker='X', s=200)
plt.title('K-Means')
plt.show()
```

---

## 8ï¸âƒ£ DECISIONES CLAVE - CHEATSHEET

| Pregunta | Respuesta |
|----------|-----------|
| **Â¿Estandarizar?** | SÃ si escalas diferentes, NO si misma escala |
| **Â¿QuÃ© mÃ©todo de enlace?** | Ward (mÃ¡s usado), o Complete para clusters compactos |
| **Â¿Jaccard o Simple Matching?** | Jaccard si 0-0 no importan, Simple Matching si sÃ­ |
| **Â¿CÃ³mo elegir k?** | Silueta + inspecciÃ³n visual + interpretabilidad |
| **Â¿JerÃ¡rquico o K-means?** | JerÃ¡rquico para explorar, K-means si sabes k |
| **Â¿QuÃ© distancia usar?** | Euclidiana (mÃ¡s comÃºn), Jaccard para binarios |

---

## 9ï¸âƒ£ FLUJO COMPLETO DEL PROCESO

```
1. DATOS ORIGINALES
        â†“
2. Â¿NumÃ©ricas o binarias?
        â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â†“                   â†“
NumÃ©ricas          Binarias
   â†“                   â†“
Â¿Escalas           Similitud
diferentes?        (Jaccard/Simple Matching)
   â†“                   â†“
SÃ â†’ Estandarizar  Convertir a distancia
   â†“                   â†“
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
3. CAPACIDAD DE CALCULAR DISTANCIAS
        â†“
4. ELIGE MÃ‰TODO DE CLUSTERING
        â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â†“                       â†“
JerÃ¡rquico           No jerÃ¡rquico
   â†“                       â†“
â”Œâ”€â”€â”´â”€â”€â”               â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
â†“     â†“               â†“         â†“
Aglom Divis       K-means    DBSCAN
   â†“                   â†“
MÃ©todo de enlace   Elegir k
(single, complete, MÃ©todo del codo
average, ward)     o Silueta
   â†“                   â†“
Dendrograma        Visualizar
   â†“                   â†“
Elegir k           Evaluar
   â†“                   â†“
Asignar clusters   Resultados
```

---

## ğŸ”Ÿ MEDIDAS DE SIMILITUD PARA DATOS BINARIOS

### **Coeficiente de Jaccard** (el mÃ¡s usado)

**FÃ³rmula:** `s = a / (a + b + c)`

- **a**: coincidencias 1-1
- **b**: discrepancias (0-1)
- **c**: discrepancias (1-0)
- **Ignora d** (coincidencias 0-0)

**CuÃ¡ndo usar:** Especies en sitios, productos comprados, palabras en documentos (cuando NO tener algo en comÃºn NO significa similitud)

```python
distancias = pdist(X_binario, metric='jaccard')
```

---

### **Simple Matching**

**FÃ³rmula:** `s = (a + d) / p`

- **Considera d** (coincidencias 0-0)

**CuÃ¡ndo usar:** SÃ­ntomas mÃ©dicos, caracterÃ­sticas de casa, test binarios (cuando ausencias compartidas SÃ son informativas)

```python
def simple_matching_distance(u, v):
    coincidencias = np.sum(u == v)
    similitud = coincidencias / len(u)
    return 1 - similitud

distancias = pdist(X_binario, metric=simple_matching_distance)
```

---

### **Tabla de contingencia**

Cuando comparas dos objetos i y k:

```
           Objeto k
        1     0    Total
i   1   a     b    a+b
    0   c     d    c+d
Total  a+c   b+d    p
```

**ConversiÃ³n de similitud a distancia:**

```python
d = sqrt(2 * (1 - s))
```

---

## 1ï¸âƒ£1ï¸âƒ£ EJEMPLO COMPLETO PASO A PASO

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

# ========== DATOS ==========
datos = {
    'ID': ['A', 'B', 'C', 'D', 'E'],
    'Edad': [25, 27, 45, 50, 52],
    'Salario': [30000, 32000, 60000, 65000, 70000]
}
df = pd.DataFrame(datos)
print(df.describe())  # Verificar escalas

# ========== PREPARAR ==========
X = df[['Edad', 'Salario']].values
labels = df['ID'].values

# Estandarizar (escalas muy diferentes)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ========== CLUSTERING ==========
Z = linkage(X_scaled, method='ward')

# ========== ELEGIR k CON SILUETA ==========
print("\nCoeficientes de Silueta:")
for k in range(2, 5):
    clusters = fcluster(Z, t=k, criterion='maxclust')
    score = silhouette_score(X_scaled, clusters)
    print(f'k={k}: {score:.3f}')

# ========== VISUALIZAR CON k=2 ==========
k = 2
distancia_corte = (Z[-(k-1), 2] + Z[-k, 2]) / 2

plt.figure(figsize=(10, 6))
dendrogram(Z, 
          labels=labels,
          color_threshold=distancia_corte,
          above_threshold_color='gray')
plt.axhline(y=distancia_corte, color='red', linestyle='--', linewidth=2)
plt.title(f'Dendrograma - {k} Clusters (Ward)')
plt.ylabel('Distancia')
plt.show()

# ========== ASIGNAR Y MOSTRAR ==========
clusters = fcluster(Z, t=k, criterion='maxclust')
df['Cluster'] = clusters
print("\nAsignaciÃ³n de clusters:")
print(df)
```

---

**FIN DEL COMPENDIO** ğŸ“˜

---

**Autor:** Claude (Anthropic)  
**Fecha:** Noviembre 2025  
**Tema:** AnÃ¡lisis de Conglomerados en Python  
**LibrerÃ­as:** scipy, scikit-learn, pandas, numpy, matplotlib
